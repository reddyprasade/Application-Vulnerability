<<<<<<< HEAD
#!/usr/bin/python3
# -*- coding: utf-8 -*-
# Coded By Kuduxaaa
"""
import os
from queue import Queue
from threading import Thread

from bs4 import BeautifulSoup
from datetime import datetime

import json
import requests

results = []
concurrent = 50
counter = 0
q = None


def save(data, finall=False) -> bool:
    if finall:
        with open('parsed_exploits.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            f.close()
        return True
    else:
        results.append(data)
        return True


def do_work():
    while True:
        url = q.get()
        res = get_response(url)
        cve_id = str(url).split('cve_id=')[1]
        if 'Unknown CVE ID' not in res.text:
            handler(res, cve_id)
        q.task_done()


def get_response(url):
    global counter
    try:
        res = requests.get(url)
        return res
    except Exception as e:
        print(e)
        pass


def handler(res, cve_id):
    try:
        data = dict()
        soup = BeautifulSoup(res.text, 'html.parser')
        td = [str(x.text).replace('\n', ' ').replace('\t', '').strip() for x in soup.findAll('td')]
        # print(soup)
        data['cve_id'] = cve_id
        data['cve_score'] = str(soup.find('div', {'class': 'cvssbox'}).text).strip()
        data['cve_ulnerability_type'] = td[23]
        data['cve_description'] = str(
            soup.find('div', {
                'class': 'cvedetailssummary'
            }).text) \
            .replace('\n', ' ') \
            .replace('\t', '') \
            .strip()

        data['authentication'] = td[21]
        data['access_complexity'] = td[20]
        data['gained_access'] = td[22]
        data['cwe_id'] = td[24]
        data['references'] = list()

        for y in [x.findAll('a') for x in soup.findAll('td', {'class': 'r_average'})]:
            reference = str(y[0].get('href'))
            data['references'].append(reference)

        if save(data):
            print(f'[+] {cve_id} Successfully Saved')
            return True
        else:
            return False
    except Exception as e:
        print(e)
        return False


def run():
    global q
    q = Queue(concurrent * 2)
    for _ in range(concurrent):
        try:
            t = Thread(target=do_work)
            t.daemon = True
            t.start()

        except Exception as e:
            print(e)
            continue

    try:
        for year in range(2000, int(datetime.now().year) + 1):
            prefix = 'https://www.cvedetails.com/cve-details.php?cve_id=CVE-' + str(year) + '-{:04n}'
            cves = [prefix.format(i) for i in range(1, 10000)]
            for cve in cves:
                q.put(cve.strip())

            q.join()
            save(results, finall=True)
    except KeyboardInterrupt:
        save(results, finall=True)


if __name__ == '__main__':
    run()
"""
import argparse
import datetime
import os
import math

import requests as req
import pandas as pd
from bs4 import BeautifulSoup
from tqdm import tqdm

WEBSITE_URL = "https://www.cvedetails.com"

CVES_DICT = {
    "cve_id": [],
    "cve_href": [],
    "cwe_id": [],
    "vuln_type": [],
    "publish_date": [],
    "update_date": [],
    "score": [],
    "acces_level": [],
    "access": [],
    "complexity": [],
    "authentication": [],
    "confidentiality": [],
    "integrity": [],
    "availability": [],
    "summary": [],
    "refs": [],
}


def get_refs(cve_url):
    cve_details_page = req.get(cve_url)
    soup = BeautifulSoup(cve_details_page.content, "html.parser")
    refs_table = soup.find(id="vulnrefstable")
    if refs_table:
        refs = refs_table.find_all("a")
        return set(ref["href"].strip() for ref in refs)
    else:
        return set()


def load_data(file):
    if os.path.exists(file):
        return pd.read_csv(file)
    else:
        return pd.DataFrame.from_dict(CVES_DICT)


def get_pagination(url, soup):
    pagination = soup.find(id="pagingb")
    pages = ["{}{}".format(url, page["href"]) for page in pagination.find_all("a")]
    return pages


def get_pages(year, page_start):
    page = req.get(
        "{}/vulnerability-list/year-{}/vulnerabilities.html".format(WEBSITE_URL, year)
    )
    soup = BeautifulSoup(page.content, "html.parser")

    return get_pagination(WEBSITE_URL, soup)[page_start::]


def get_cves_rows(soup):
    cves_list = soup.find(id="searchresults")
    cves_rows = cves_list.find_all("tr")
    return cves_rows


def parse_cves_rows(rows):
    for key in CVES_DICT:
        CVES_DICT[key] = []
    for i in range(1, len(rows[1::]), 2):
        tds = rows[i].find_all("td")
        CVES_DICT["cve_id"].append(tds[1].a.getText().strip())
        CVES_DICT["cve_href"].append(
            "{}{}".format(WEBSITE_URL, tds[1].a["href"].strip())
        )
        CVES_DICT["cwe_id"].append(
            "CWE-{}".format(tds[2].a.getText().strip()) if tds[2].a != None else ""
        )
        CVES_DICT["vuln_type"].append(tds[4].getText().strip())
        CVES_DICT["publish_date"].append(tds[5].getText().strip())
        CVES_DICT["update_date"].append(tds[6].getText().strip())
        CVES_DICT["score"].append(tds[7].div.getText().strip())
        CVES_DICT["acces_level"].append(tds[8].getText().strip())
        CVES_DICT["access"].append(tds[9].getText().strip())
        CVES_DICT["complexity"].append(tds[10].getText().strip())
        CVES_DICT["authentication"].append(tds[11].getText().strip())
        CVES_DICT["confidentiality"].append(tds[12].getText().strip())
        CVES_DICT["integrity"].append(tds[13].getText().strip())
        CVES_DICT["availability"].append(tds[14].getText().strip())
        CVES_DICT["summary"].append(rows[i + 1].find_all("td")[0].getText().strip())
        CVES_DICT["refs"].append(
            get_refs("{}{}".format(WEBSITE_URL, tds[1].a["href"].strip()))
        )
    return CVES_DICT


def get_cves_info(page):
    cves_list_page = req.get(page.strip())
    soup = BeautifulSoup(cves_list_page.content, "html.parser")
    rows = get_cves_rows(soup)
    return parse_cves_rows(rows)


def scrape_cve_details(year, folder):

    if not os.path.exists(folder):
        os.mkdir(folder)

    f_path = f"{folder}{year}.csv"
    df = load_data(f_path)
    dist = (len(df) / 50) - int(len(df) / 50)
    print("dist:", dist)
    if dist == 0:
        page_start = int(len(df) / 50)
    else:
        page_start = math.ceil(len(df) / 50)
    print(f"Starting at page {page_start}")
    pages = get_pages(year, page_start)
    for page in tqdm(pages):
        print(f"Scraping page: {page}")
        cves = get_cves_info(page)
        df = pd.concat([df, pd.DataFrame.from_dict(cves)], ignore_index=True)
        df.to_csv(f_path, index=False)


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description="Web Scraping Tool for CVE Details website (https://www.cvedetails.com/):"
    )
    parser.add_argument("--mode", dest="format", choices=["year"])
    parser.add_argument("--year", type=str, metavar="year", help="year of cves")
    parser.add_argument(
        "--folder",
        type=str,
        metavar="output folder",
        help="folder where you want to save the results",
    )

    args = parser.parse_args()

    now = datetime.datetime.now()
    years = (year for year in range(1999, now.year + 1))

    if args.format == "year":
        if args.folder and int(args.year) in years:
            scrape_cve_details(args.year, args.folder)
        else:
            print("Something wrong with the output file name or year.")
    else:
        print("Mode is wrong. year available.")
=======
#!/usr/bin/python3
# -*- coding: utf-8 -*-
# Coded By Kuduxaaa
"""
import os
from queue import Queue
from threading import Thread

from bs4 import BeautifulSoup
from datetime import datetime

import json
import requests

results = []
concurrent = 50
counter = 0
q = None


def save(data, finall=False) -> bool:
    if finall:
        with open('parsed_exploits.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            f.close()
        return True
    else:
        results.append(data)
        return True


def do_work():
    while True:
        url = q.get()
        res = get_response(url)
        cve_id = str(url).split('cve_id=')[1]
        if 'Unknown CVE ID' not in res.text:
            handler(res, cve_id)
        q.task_done()


def get_response(url):
    global counter
    try:
        res = requests.get(url)
        return res
    except Exception as e:
        print(e)
        pass


def handler(res, cve_id):
    try:
        data = dict()
        soup = BeautifulSoup(res.text, 'html.parser')
        td = [str(x.text).replace('\n', ' ').replace('\t', '').strip() for x in soup.findAll('td')]
        # print(soup)
        data['cve_id'] = cve_id
        data['cve_score'] = str(soup.find('div', {'class': 'cvssbox'}).text).strip()
        data['cve_ulnerability_type'] = td[23]
        data['cve_description'] = str(
            soup.find('div', {
                'class': 'cvedetailssummary'
            }).text) \
            .replace('\n', ' ') \
            .replace('\t', '') \
            .strip()

        data['authentication'] = td[21]
        data['access_complexity'] = td[20]
        data['gained_access'] = td[22]
        data['cwe_id'] = td[24]
        data['references'] = list()

        for y in [x.findAll('a') for x in soup.findAll('td', {'class': 'r_average'})]:
            reference = str(y[0].get('href'))
            data['references'].append(reference)

        if save(data):
            print(f'[+] {cve_id} Successfully Saved')
            return True
        else:
            return False
    except Exception as e:
        print(e)
        return False


def run():
    global q
    q = Queue(concurrent * 2)
    for _ in range(concurrent):
        try:
            t = Thread(target=do_work)
            t.daemon = True
            t.start()

        except Exception as e:
            print(e)
            continue

    try:
        for year in range(2000, int(datetime.now().year) + 1):
            prefix = 'https://www.cvedetails.com/cve-details.php?cve_id=CVE-' + str(year) + '-{:04n}'
            cves = [prefix.format(i) for i in range(1, 10000)]
            for cve in cves:
                q.put(cve.strip())

            q.join()
            save(results, finall=True)
    except KeyboardInterrupt:
        save(results, finall=True)


if __name__ == '__main__':
    run()
"""
import argparse
import datetime
import os
import math

import requests as req
import pandas as pd
from bs4 import BeautifulSoup
from tqdm import tqdm

WEBSITE_URL = "https://www.cvedetails.com"

CVES_DICT = {
    "cve_id": [],
    "cve_href": [],
    "cwe_id": [],
    "vuln_type": [],
    "publish_date": [],
    "update_date": [],
    "score": [],
    "acces_level": [],
    "access": [],
    "complexity": [],
    "authentication": [],
    "confidentiality": [],
    "integrity": [],
    "availability": [],
    "summary": [],
    "refs": [],
}


def get_refs(cve_url):
    cve_details_page = req.get(cve_url)
    soup = BeautifulSoup(cve_details_page.content, "html.parser")
    refs_table = soup.find(id="vulnrefstable")
    if refs_table:
        refs = refs_table.find_all("a")
        return set(ref["href"].strip() for ref in refs)
    else:
        return set()


def load_data(file):
    if os.path.exists(file):
        return pd.read_csv(file)
    else:
        return pd.DataFrame.from_dict(CVES_DICT)


def get_pagination(url, soup):
    pagination = soup.find(id="pagingb")
    pages = ["{}{}".format(url, page["href"]) for page in pagination.find_all("a")]
    return pages


def get_pages(year, page_start):
    page = req.get(
        "{}/vulnerability-list/year-{}/vulnerabilities.html".format(WEBSITE_URL, year)
    )
    soup = BeautifulSoup(page.content, "html.parser")

    return get_pagination(WEBSITE_URL, soup)[page_start::]


def get_cves_rows(soup):
    cves_list = soup.find(id="searchresults")
    cves_rows = cves_list.find_all("tr")
    return cves_rows


def parse_cves_rows(rows):
    for key in CVES_DICT:
        CVES_DICT[key] = []
    for i in range(1, len(rows[1::]), 2):
        tds = rows[i].find_all("td")
        CVES_DICT["cve_id"].append(tds[1].a.getText().strip())
        CVES_DICT["cve_href"].append(
            "{}{}".format(WEBSITE_URL, tds[1].a["href"].strip())
        )
        CVES_DICT["cwe_id"].append(
            "CWE-{}".format(tds[2].a.getText().strip()) if tds[2].a != None else ""
        )
        CVES_DICT["vuln_type"].append(tds[4].getText().strip())
        CVES_DICT["publish_date"].append(tds[5].getText().strip())
        CVES_DICT["update_date"].append(tds[6].getText().strip())
        CVES_DICT["score"].append(tds[7].div.getText().strip())
        CVES_DICT["acces_level"].append(tds[8].getText().strip())
        CVES_DICT["access"].append(tds[9].getText().strip())
        CVES_DICT["complexity"].append(tds[10].getText().strip())
        CVES_DICT["authentication"].append(tds[11].getText().strip())
        CVES_DICT["confidentiality"].append(tds[12].getText().strip())
        CVES_DICT["integrity"].append(tds[13].getText().strip())
        CVES_DICT["availability"].append(tds[14].getText().strip())
        CVES_DICT["summary"].append(rows[i + 1].find_all("td")[0].getText().strip())
        CVES_DICT["refs"].append(
            get_refs("{}{}".format(WEBSITE_URL, tds[1].a["href"].strip()))
        )
    return CVES_DICT


def get_cves_info(page):
    cves_list_page = req.get(page.strip())
    soup = BeautifulSoup(cves_list_page.content, "html.parser")
    rows = get_cves_rows(soup)
    return parse_cves_rows(rows)


def scrape_cve_details(year, folder):

    if not os.path.exists(folder):
        os.mkdir(folder)

    f_path = f"{folder}{year}.csv"
    df = load_data(f_path)
    dist = (len(df) / 50) - int(len(df) / 50)
    print("dist:", dist)
    if dist == 0:
        page_start = int(len(df) / 50)
    else:
        page_start = math.ceil(len(df) / 50)
    print(f"Starting at page {page_start}")
    pages = get_pages(year, page_start)
    for page in tqdm(pages):
        print(f"Scraping page: {page}")
        cves = get_cves_info(page)
        df = pd.concat([df, pd.DataFrame.from_dict(cves)], ignore_index=True)
        df.to_csv(f_path, index=False)


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description="Web Scraping Tool for CVE Details website (https://www.cvedetails.com/):"
    )
    parser.add_argument("--mode", dest="format", choices=["year"])
    parser.add_argument("--year", type=str, metavar="year", help="year of cves")
    parser.add_argument(
        "--folder",
        type=str,
        metavar="output folder",
        help="folder where you want to save the results",
    )

    args = parser.parse_args()

    now = datetime.datetime.now()
    years = (year for year in range(1999, now.year + 1))

    if args.format == "year":
        if args.folder and int(args.year) in years:
            scrape_cve_details(args.year, args.folder)
        else:
            print("Something wrong with the output file name or year.")
    else:
        print("Mode is wrong. year available.")
>>>>>>> 9a4bb3460df188f51574a11fd5f63b912c8c39f0
